{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deployment_mercari.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzLNA9LrNyt0"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "from tqdm import tqdm\r\n",
        "from tqdm import tqdm_notebook\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "from keras.layers import Input, Embedding, LSTM, Dropout, BatchNormalization, Dense, concatenate, Flatten, Conv1D, MaxPool1D, LeakyReLU, ELU, SpatialDropout1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\r\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Model, load_model\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRHQiwX2wZFR"
      },
      "source": [
        "#1.Loading **Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhnGy4QmPKnB",
        "outputId": "a3893676-becf-4ed2-b172-eef0acb9b1e8"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9WKEgNGWZk0",
        "outputId": "7404b8a4-8d41-462d-b809-5dfee9fefd19"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cybMNeoOArC5",
        "outputId": "487e39fd-87de-4ffd-eba1-9a984412f84d"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('all') \r\n",
        "stopwords=set(stopwords.words('english'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d0bV7J6-bu0"
      },
      "source": [
        "def fill_missing_values(df):\r\n",
        "    \r\n",
        "    df['name']=df['name'].fillna('none')\r\n",
        "    df['item_description']=df['item_description'].fillna('none')\r\n",
        "    df['brand_name']=df['brand_name'].fillna('unknown')\r\n",
        "    df['category_name']=df['category_name'].fillna('none')\r\n",
        "    return df\r\n",
        "def split_text(text):\r\n",
        "    if text=='none':\r\n",
        "        return [\"unknown\", \"unknown\", \"unknown\"]\r\n",
        "    return text.split(\"/\")\r\n",
        "\r\n",
        "def split_categories(df):\r\n",
        "    df['main_cat'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].apply(lambda x: split_text(x)))\r\n",
        "    df = df.drop('category_name', axis=1)\r\n",
        "    return df\r\n",
        "def counting_stopwords(data):\r\n",
        "    count_words=[]\r\n",
        "    for i in data['item_description']:\r\n",
        "        count=0\r\n",
        "        for j in i.split(' '):\r\n",
        "            if j in stopwords:\r\n",
        "                count+=1\r\n",
        "        count_words.append(count)\r\n",
        "    data['count_of_stopwords']=count_words\r\n",
        "    return data\r\n",
        "# ref - www.appliedaicourse.com/\r\n",
        "''' This code performs text processing by cleaning text including \r\n",
        "removing stopwords, reemoving special characters, performing, word decontraction etc. '''\r\n",
        "\r\n",
        "def decontracted(phrase):\r\n",
        "    # specific\r\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\r\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\r\n",
        "\r\n",
        "    # general\r\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\r\n",
        "    return phrase\r\n",
        "\r\n",
        "# https://gist.github.com/sebleier/554280\r\n",
        "# we are removing the words from the stop words list: 'no', 'nor', 'not'\r\n",
        "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\r\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\r\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\r\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\r\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\r\n",
        "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\r\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\r\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\r\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\r\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\r\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\r\n",
        "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\r\n",
        "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\r\n",
        "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\r\n",
        "            'won', \"won't\", 'wouldn', \"wouldn't\"]\r\n",
        "\r\n",
        "def text_preprocess(data):\r\n",
        "    preprocessed = []\r\n",
        "    # tqdm is for printing the status bar\r\n",
        "    for sentance in data:\r\n",
        "        sent = decontracted(sentance)\r\n",
        "        sent = sent.replace('\\\\r', ' ')\r\n",
        "        sent = sent.replace('\\\\\"', ' ')\r\n",
        "        sent = sent.replace('\\\\n', ' ')\r\n",
        "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\r\n",
        "        # https://gist.github.com/sebleier/554280\r\n",
        "        sent = ' '.join(e for e in sent.split() if e not in stopwords)\r\n",
        "        preprocessed.append(sent.lower().strip())\r\n",
        "\r\n",
        "    return preprocessed\r\n",
        "def get_item_des_len(data):\r\n",
        "    data['length']=data['item_description'].str.len()\r\n",
        "    return data\r\n",
        "    \r\n",
        "def brand_name_category(data):\r\n",
        "    cat_brandname=[]\r\n",
        "    for i in data['brand_name']:\r\n",
        "        if i!='unknown':\r\n",
        "            cat_brandname.append(1)\r\n",
        "        else:\r\n",
        "            cat_brandname.append(0)\r\n",
        "    data['brand_name_present']=cat_brandname\r\n",
        "\r\n",
        "    return data"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTTvpLEEW0v7"
      },
      "source": [
        "\r\n",
        "def fdata_pipeline(df):\r\n",
        "    #this will do all the necessary preprocessing\r\n",
        "    print()\r\n",
        "    # print(\"Filling missing values...\")\r\n",
        "    df = fill_missing_values(df)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "    # print(\"Splitting categories...\")\r\n",
        "    df = split_categories(df)\r\n",
        "\r\n",
        "    # print('counting stop words.....')\r\n",
        "    df=counting_stopwords(df)\r\n",
        "\r\n",
        "    # print(\"pre-processing text data...\")\r\n",
        "    df['preprocessed_item_des'] = text_preprocess(df['item_description'])\r\n",
        "\r\n",
        "\r\n",
        "    # print('Getting word lengths')\r\n",
        "    df=get_item_des_len(df)\r\n",
        "    # print('brand_name_present')\r\n",
        "    df=brand_name_category(df)\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "    return df"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0eBPHsh-q4X"
      },
      "source": [
        "def text_embeddings(text, tokenizer, max_len_doc):\r\n",
        "    # Word Tokenizer\r\n",
        "    encoded_docs_train = tokenizer.texts_to_sequences(text)\r\n",
        "    text_padded = pad_sequences(encoded_docs_train, maxlen=max_len_doc, padding='post')\r\n",
        "\r\n",
        "    return text_padded\r\n",
        "\r\n",
        "    \r\n",
        "def categorical_embeddings(cat_data, le):\r\n",
        "    cat_data = cat_data.map(lambda s: '<unknown>' if s not in le.classes_ else s)\r\n",
        "    le.classes_ = np.append(le.classes_, '<unknown>')\r\n",
        "    encoded_cat = le.transform(cat_data.values)\r\n",
        "    \r\n",
        "    return encoded_cat"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQdye35tXLaR"
      },
      "source": [
        "def pre_process_convlstm_input(df, desc_tokenizer, name_tokenizer, desc_max_len_doc, name_max_len_doc, brd_nm, sca_subcat1, sca_subcat2, sca_subcat3,scr_length,\r\n",
        "                   scr_stopw,itm_conid,scr_bnpresent,scr_ship):\r\n",
        "   #textdata embedding\r\n",
        "    desc_text_padded = text_embeddings(df['preprocessed_item_des'].apply(str), desc_tokenizer, desc_max_len_doc)\r\n",
        "    name_text_padded = text_embeddings(df['name'].apply(str), name_tokenizer, name_max_len_doc)\r\n",
        "   \r\n",
        "\r\n",
        "\r\n",
        "    #categorical data\r\n",
        "    bn_cat = categorical_embeddings(df['brand_name'], brd_nm)\r\n",
        "    main_cat = categorical_embeddings(df['main_cat'], sca_subcat1)\r\n",
        "    sc2_cat = categorical_embeddings(df['subcat_1'], sca_subcat2)\r\n",
        "    sc3_cat = categorical_embeddings(df['subcat_2'], sca_subcat3)\r\n",
        "    \r\n",
        "    #standard scaler\r\n",
        "    a=scr_length.transform(np.array(df['length']).reshape(-1,1))\r\n",
        "    b=scr_stopw.transform(np.array(df['count_of_stopwords']).reshape(-1,1))\r\n",
        "    c=itm_conid.transform(np.array(df['item_condition_id']).reshape(-1,1))\r\n",
        "    d=scr_bnpresent.transform(np.array(df['brand_name_present']).reshape(-1,1))\r\n",
        "    e=scr_ship.transform(np.array(df['shipping']).reshape(-1,1))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    #converting above to numerical data\r\n",
        "    X_numerical_feat=np.concatenate((a,b,c,d,e),axis=1)\r\n",
        "\r\n",
        "    test_full =[name_text_padded,desc_text_padded,main_cat,sc2_cat,sc3_cat,bn_cat,X_numerical_feat]\r\n",
        "\r\n",
        "    \r\n",
        "    return test_full\r\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsnDCupdXVX0"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "model_best=tf.keras.models.load_model('/content/drive/MyDrive/pavn')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCxJCEtlXVQn"
      },
      "source": [
        "def take_main_input(X):\r\n",
        "    df = pd.DataFrame(X)\r\n",
        "    df.columns = ['name', 'item_condition_id', 'category_name', 'brand_name', 'shipping', 'item_description']\r\n",
        "    \r\n",
        "    #loading all the pickel files from model\r\n",
        "    with open('/content/drive/MyDrive/pavn/tokenizer_main.pickle', 'rb') as handle:\r\n",
        "        tokenizer_main = pickle.load(handle)\r\n",
        "    with open('/content/drive/MyDrive/pavn/tokenizer1.pickle', 'rb') as handle:\r\n",
        "        tokenizer_1 = pickle.load(handle)\r\n",
        "    from pickle import load\r\n",
        "    sca_subcat1 = load(open('/content/drive/MyDrive/pavn/scalersubcat1.pkl', 'rb'))\r\n",
        "    sca_subcat2 = load(open('/content/drive/MyDrive/pavn/scalersubcat2.pkl', 'rb'))\r\n",
        "    sca_subcat3 = load(open('/content/drive/MyDrive/pavn/scalersubcat3.pkl', 'rb'))\r\n",
        "    brd_nm = load(open('/content/drive/MyDrive/pavn/brand_name.pkl', 'rb'))\r\n",
        "    #################################################\r\n",
        "    scr_stopw = load(open('/content/drive/MyDrive/pavn/scalar_stopwords.pkl', 'rb'))\r\n",
        "    itm_conid = load(open('/content/drive/MyDrive/pavn/scalar_item_con_id.pkl', 'rb'))\r\n",
        "    scr_ship = load(open('/content/drive/MyDrive/pavn/scalar_shipping.pkl', 'rb'))\r\n",
        "    scr_bnpresent = load(open('/content/drive/MyDrive/pavn/scalar_brandname_pres.pkl', 'rb'))\r\n",
        "    scr_length=load(open('/content/drive/MyDrive/pavn/scalar_length.pkl','rb'))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    df = fdata_pipeline(df)\r\n",
        "    #max length of 253 for item desc and 17 for name is got from the model\r\n",
        "\r\n",
        "    test_full=pre_process_convlstm_input(df,tokenizer_1,tokenizer_main,253,17,brd_nm,sca_subcat1,sca_subcat2,sca_subcat3,scr_length,scr_stopw,itm_conid,scr_bnpresent,scr_ship)\r\n",
        "    \r\n",
        "\r\n",
        "    pred=model_best.predict(test_full)\r\n",
        "    \r\n",
        "   \r\n",
        "\r\n",
        "    pred = np.exp(pred)\r\n",
        "    return pred"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p-FDBcxXVD4",
        "outputId": "e538e8b7-3afc-49d4-84e4-bb14765824b7"
      },
      "source": [
        "prediction = take_main_input([[\"MLB Cincinnati Reds T Shirt Size XL\", 3, \"Men/Tops/T-shirts\", np.nan, 1, \"No description yet\"]])\r\n",
        "print(\"The product price estimation is $ {}\".format(prediction.tolist()[0][0]))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The product price estimation is $ 14.763649940490723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SWAysNCrk-l"
      },
      "source": [
        ""
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThC_snBH0wPG",
        "outputId": "a0a08803-162c-4557-b28a-d5a9d3943b39"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HohyDyy07QyV"
      },
      "source": [
        "import flask"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlVOxxGq0wT2"
      },
      "source": [
        "from flask import Flask\r\n",
        "from flask import request\r\n",
        "from flask_ngrok import run_with_ngrok"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ndgr4q0wX0",
        "outputId": "137fd969-8692-4d1f-9517-39ea4c32e6ab"
      },
      "source": [
        "from flask import  Flask,jsonify,request\r\n",
        "app = Flask(__name__)\r\n",
        "run_with_ngrok(app)\r\n",
        "\r\n",
        "\r\n",
        "@app.route('/')\r\n",
        "def hello_world():\r\n",
        "    return 'Hello World!'\r\n",
        "\r\n",
        "\r\n",
        "@app.route('/index')\r\n",
        "def index():\r\n",
        "    return flask.render_template('index.html')\r\n",
        "\r\n",
        "\r\n",
        "@app.route('/predict', methods=['POST'])\r\n",
        "def predict():\r\n",
        "    \r\n",
        "    \r\n",
        "\r\n",
        "    to_predict_list = request.form.to_dict()\r\n",
        "    to_predict_list = list(to_predict_list.values())\r\n",
        "    print(to_predict_list)\r\n",
        "    \r\n",
        "    \r\n",
        "    lst1=to_predict_list\r\n",
        "    lst2=[]\r\n",
        "    for i in range(len(lst1)):\r\n",
        "        if i==0:\r\n",
        "            lst2.append(lst1[0])\r\n",
        "        if i==1:\r\n",
        "            lst2.append(int(lst1[1]))\r\n",
        "            \r\n",
        "        if i==2:\r\n",
        "            lst2.append(lst1[2])\r\n",
        "        if i==3:\r\n",
        "            lst2.append(lst1[3])\r\n",
        "        if i==4:\r\n",
        "            lst2.append(int(lst1[4]))\r\n",
        "        if i==5:\r\n",
        "            lst2.append(lst1[5])\r\n",
        "            \r\n",
        "    print(lst2)\r\n",
        "    \r\n",
        "    prediction11 = take_main_input([lst2])\r\n",
        "    # print(prediction11.tolist()[0][0])\r\n",
        "\r\n",
        "    return flask.render_template('index.html', prediction_text='The price of the product estimated to be $ {}'.format(prediction11.tolist()[0][0]))\r\n",
        "\r\n",
        "    \r\n",
        "app.run()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://8579ee9bf895.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/Jan/2021 20:33:35] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [23/Jan/2021 20:33:36] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [23/Jan/2021 20:33:39] \"\u001b[37mGET /index HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['AVA-VIV Blouse', '1', 'Women/Tops &Blouses/Blouse', 'Target', '1', 'Adorable top with a hint of lace and a key hole']\n",
            "['AVA-VIV Blouse', 1, 'Women/Tops &Blouses/Blouse', 'Target', 1, 'Adorable top with a hint of lace and a key hole']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [23/Jan/2021 20:33:54] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thXr3wlfww_i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}